# utils/text_utils.py

import re
import unicodedata
import hashlib
import random
import string
import jieba
from typing import List, Dict, Tuple, Optional, Set, Any, Union
import json
from collections import Counter
import difflib
from src.utils.logger import setup_logger

logger = setup_logger(__name__)

class TextNormalizer:
    """æ–‡æœ¬æ ‡å‡†åŒ–å·¥å…·"""
    
    @staticmethod
    def normalize_unicode(text: str, form: str = "NFKC") -> str:
        """Unicodeæ ‡å‡†åŒ–"""
        return unicodedata.normalize(form, text)
    
    @staticmethod
    def remove_extra_spaces(text: str) -> str:
        """ç§»é™¤å¤šä½™ç©ºæ ¼"""
        # æ›¿æ¢å¤šä¸ªç©ºæ ¼ä¸ºå•ä¸ªç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        # ç§»é™¤é¦–å°¾ç©ºæ ¼
        return text.strip()
    
    @staticmethod
    def remove_html_tags(text: str) -> str:
        """ç§»é™¤HTMLæ ‡ç­¾"""
        return re.sub(r'<[^>]+>', '', text)
    
    @staticmethod
    def remove_urls(text: str) -> str:
        """ç§»é™¤URL"""
        return re.sub(r'https?://\S+|www\.\S+', '', text)
    
    @staticmethod
    def remove_emojis(text: str) -> str:
        """ç§»é™¤è¡¨æƒ…ç¬¦å·"""
        emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # è¡¨æƒ…ç¬¦å·
            "\U0001F300-\U0001F5FF"  # ç¬¦å·å’Œè±¡å½¢æ–‡å­—
            "\U0001F680-\U0001F6FF"  # äº¤é€šå’Œåœ°å›¾ç¬¦å·
            "\U0001F700-\U0001F77F"  # ç‚¼é‡‘æœ¯ç¬¦å·
            "\U0001F780-\U0001F7FF"  # å‡ ä½•å½¢çŠ¶
            "\U0001F800-\U0001F8FF"  # è¡¥å……ç®­å¤´
            "\U0001F900-\U0001F9FF"  # è¡¥å……ç¬¦å·å’Œè±¡å½¢æ–‡å­—
            "\U0001FA00-\U0001FA6F"  # å›½é™…è±¡æ£‹ç¬¦å·
            "\U0001FA70-\U0001FAFF"  # ç¬¦å·å’Œè±¡å½¢æ–‡å­—æ‰©å±•
            "\U00002702-\U000027B0"  # è£…é¥°ç¬¦å·
            "\U000024C2-\U0001F251" 
            "]+"
        )
        return emoji_pattern.sub('', text)
    
    @staticmethod
    def normalize_whitespace(text: str) -> str:
        """æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦"""
        # å°†å„ç§ç©ºç™½å­—ç¬¦ï¼ˆåˆ¶è¡¨ç¬¦ã€æ¢è¡Œç¬¦ç­‰ï¼‰è½¬æ¢ä¸ºç©ºæ ¼
        text = re.sub(r'[\s\t\n\r\f\v]+', ' ', text)
        return text.strip()
    
    @staticmethod
    def normalize_punctuation(text: str) -> str:
        """æ ‡å‡†åŒ–æ ‡ç‚¹ç¬¦å·"""
        # ç»Ÿä¸€ä¸­æ–‡å’Œè‹±æ–‡æ ‡ç‚¹
        punctuation_map = {
            'ï¼Œ': ',',
            'ã€‚': '.',
            'ï¼': '!',
            'ï¼Ÿ': '?',
            'ï¼›': ';',
            'ï¼š': ':',
            '"': '"',
            '"': '"',
            ''': "'",
            ''': "'",
            'ï¼ˆ': '(',
            'ï¼‰': ')',
            'ã€': '[',
            'ã€‘': ']',
            'ã€Š': '<',
            'ã€‹': '>',
            'â€”': '-'
        }
        
        for cn_punct, en_punct in punctuation_map.items():
            text = text.replace(cn_punct, en_punct)
        
        return text
    
    @staticmethod
    def full_to_half_width(text: str) -> str:
        """å…¨è§’è½¬åŠè§’"""
        result = ""
        for char in text:
            code = ord(char)
            # å…¨è§’ç©ºæ ¼ç›´æ¥è½¬æ¢
            if code == 0x3000:
                code = 0x0020
            # å…¨è§’å­—ç¬¦ï¼ˆé™¤ç©ºæ ¼å¤–ï¼‰æ ¹æ®å…³ç³»è½¬æ¢
            elif 0xFF01 <= code <= 0xFF5E:
                code -= 0xFEE0
            result += chr(code)
        return result
    
    @staticmethod
    def normalize_text(
        text: str,
        lowercase: bool = True,
        remove_html: bool = True,
        remove_url: bool = True,
        remove_emoji: bool = False,
        normalize_unicode: bool = True,
        normalize_spaces: bool = True,
        normalize_punct: bool = False,
        to_half_width: bool = False
    ) -> str:
        """ç»¼åˆæ–‡æœ¬æ ‡å‡†åŒ–"""
        if normalize_unicode:
            text = TextNormalizer.normalize_unicode(text)
        
        if remove_html:
            text = TextNormalizer.remove_html_tags(text)
        
        if remove_url:
            text = TextNormalizer.remove_urls(text)
        
        if remove_emoji:
            text = TextNormalizer.remove_emojis(text)
        
        if normalize_spaces:
            text = TextNormalizer.normalize_whitespace(text)
        
        if normalize_punct:
            text = TextNormalizer.normalize_punctuation(text)
        
        if to_half_width:
            text = TextNormalizer.full_to_half_width(text)
        
        if lowercase:
            text = text.lower()
        
        return text


class TextSegmenter:
    """æ–‡æœ¬åˆ†è¯å·¥å…·"""
    
    def __init__(self, user_dict_path: Optional[str] = None):
        """åˆå§‹åŒ–åˆ†è¯å·¥å…·"""
        if user_dict_path:
            try:
                jieba.load_userdict(user_dict_path)
                logger.info(f"Loaded user dictionary from {user_dict_path}")
            except Exception as e:
                logger.error(f"Failed to load user dictionary: {e}")
    
    def segment(self, text: str, cut_all: bool = False, HMM: bool = True) -> List[str]:
        """åˆ†è¯"""
        return list(jieba.cut(text, cut_all=cut_all, HMM=HMM))
    
    def segment_precise(self, text: str) -> List[str]:
        """ç²¾ç¡®æ¨¡å¼åˆ†è¯"""
        return list(jieba.cut(text, cut_all=False))
    
    def segment_full(self, text: str) -> List[str]:
        """å…¨æ¨¡å¼åˆ†è¯"""
        return list(jieba.cut(text, cut_all=True))
    
    def segment_search(self, text: str) -> List[str]:
        """æœç´¢å¼•æ“æ¨¡å¼åˆ†è¯"""
        return list(jieba.cut_for_search(text))
    
    def extract_keywords(self, text: str, topK: int = 20, withWeight: bool = False) -> Union[List[str], List[Tuple[str, float]]]:
        """æå–å…³é”®è¯"""
        import jieba.analyse
        return jieba.analyse.extract_tags(text, topK=topK, withWeight=withWeight)
    
    def extract_tfidf_keywords(self, text: str, topK: int = 20, withWeight: bool = False) -> Union[List[str], List[Tuple[str, float]]]:
        """ä½¿ç”¨TFIDFæå–å…³é”®è¯"""
        import jieba.analyse
        return jieba.analyse.extract_tags(text, topK=topK, withWeight=withWeight)
    
    def extract_textrank_keywords(self, text: str, topK: int = 20, withWeight: bool = False) -> Union[List[str], List[Tuple[str, float]]]:
        """ä½¿ç”¨TextRankæå–å…³é”®è¯"""
        import jieba.analyse
        return jieba.analyse.textrank(text, topK=topK, withWeight=withWeight)
    
    def add_word(self, word: str, freq: Optional[int] = None, tag: Optional[str] = None) -> None:
        """æ·»åŠ æ–°è¯åˆ°è¯å…¸"""
        jieba.add_word(word, freq, tag)
    
    def get_sentence_segments(self, text: str) -> List[str]:
        """è·å–å¥å­åˆ†æ®µ"""
        # ä½¿ç”¨æ ‡ç‚¹ç¬¦å·åˆ†å‰²å¥å­
        pattern = r'[ã€‚ï¼ï¼Ÿ!?]+'
        segments = re.split(pattern, text)
        return [segment.strip() for segment in segments if segment.strip()]
    
    def segment_with_pos(self, text: str) -> List[Tuple[str, str]]:
        """åˆ†è¯å¹¶è¿”å›è¯æ€§æ ‡æ³¨"""
        import jieba.posseg as pseg
        return [(w, p) for w, p in pseg.cut(text)]
    
    def get_stopwords(self, stopwords_path: str) -> Set[str]:
        """ä»æ–‡ä»¶åŠ è½½åœç”¨è¯"""
        try:
            with open(stopwords_path, 'r', encoding='utf-8') as f:
                return set([line.strip() for line in f])
        except Exception as e:
            logger.error(f"åŠ è½½åœç”¨è¯å¤±è´¥: {e}")
            return set()
    
    def remove_stopwords(self, tokens: List[str], stopwords: Set[str]) -> List[str]:
        """ç§»é™¤åœç”¨è¯"""
        return [token for token in tokens if token not in stopwords]


class TextHasher:
    """æ–‡æœ¬å“ˆå¸Œå·¥å…·"""
    
    @staticmethod
    def md5(text: str) -> str:
        """è®¡ç®—MD5å“ˆå¸Œ"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    @staticmethod
    def sha1(text: str) -> str:
        """è®¡ç®—SHA1å“ˆå¸Œ"""
        return hashlib.sha1(text.encode('utf-8')).hexdigest()
    
    @staticmethod
    def sha256(text: str) -> str:
        """è®¡ç®—SHA256å“ˆå¸Œ"""
        return hashlib.sha256(text.encode('utf-8')).hexdigest()
    
    @staticmethod
    def get_hash(text: str, algorithm: str = "md5") -> str:
        """æ ¹æ®æŒ‡å®šç®—æ³•è®¡ç®—å“ˆå¸Œ"""
        if algorithm == "md5":
            return TextHasher.md5(text)
        elif algorithm == "sha1":
            return TextHasher.sha1(text)
        elif algorithm == "sha256":
            return TextHasher.sha256(text)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å“ˆå¸Œç®—æ³•: {algorithm}")
    
    @staticmethod
    def get_simhash(text: str) -> str:
        """è®¡ç®—SimHash (ç”¨äºè¿‘ä¼¼æ–‡æœ¬é‡å¤æ£€æµ‹)"""
        try:
            import simhash
            return str(simhash.Simhash(text.split()).value)
        except ImportError:
            logger.warning("simhash æ¨¡å—æœªå®‰è£…ï¼Œæ— æ³•è®¡ç®— SimHash")
            return TextHasher.md5(text)


class TextGenerator:
    """æ–‡æœ¬ç”Ÿæˆå·¥å…·"""
    
    @staticmethod
    def generate_random_string(length: int, include_digits: bool = True, 
                             include_letters: bool = True, include_punctuation: bool = False) -> str:
        """ç”Ÿæˆéšæœºå­—ç¬¦ä¸²"""
        chars = ""
        
        if include_letters:
            chars += string.ascii_letters
        if include_digits:
            chars += string.digits
        if include_punctuation:
            chars += string.punctuation
        
        if not chars:
            raise ValueError("è‡³å°‘éœ€è¦åŒ…å«ä¸€ç§å­—ç¬¦ç±»å‹")
        
        return ''.join(random.choice(chars) for _ in range(length))
    
    @staticmethod
    def generate_uuid(as_hex: bool = True) -> str:
        """ç”ŸæˆUUID"""
        import uuid
        if as_hex:
            return uuid.uuid4().hex
        else:
            return str(uuid.uuid4())
    
    @staticmethod
    def generate_random_sentence(word_count: int = 10, lang: str = "en") -> str:
        """ç”Ÿæˆéšæœºå¥å­"""
        if lang == "en":
            # è‹±æ–‡éšæœºå•è¯
            words = ["the", "quick", "brown", "fox", "jumps", "over", "lazy", "dog",
                   "a", "an", "and", "in", "on", "at", "with", "by", "to", "from",
                   "for", "of", "is", "was", "am", "are", "were", "be", "being",
                   "been", "have", "has", "had", "do", "does", "did", "can", "could",
                   "will", "would", "shall", "should", "may", "might", "must"]
        elif lang == "zh":
            # ä¸­æ–‡éšæœºè¯
            words = ["æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ", "æˆ‘ä»¬", "ä½ ä»¬", "ä»–ä»¬",
                   "è¿™", "é‚£", "è¿™é‡Œ", "é‚£é‡Œ", "è¿™ä¸ª", "é‚£ä¸ª", "ä»Šå¤©", "æ˜å¤©",
                   "æ˜¨å¤©", "æ—©ä¸Š", "ä¸­åˆ", "æ™šä¸Š", "æ˜¥", "å¤", "ç§‹", "å†¬",
                   "å»", "æ¥", "èµ°", "è·‘", "è·³", "çœ‹", "è¯´", "å¬", "åƒ", "å–"]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¯­è¨€: {lang}")
        
        # ç”Ÿæˆéšæœºå•è¯åºåˆ—
        sentence = " ".join(random.choice(words) for _ in range(word_count))
        
        # é¦–å­—æ¯å¤§å†™å¹¶æ·»åŠ å¥å·
        if lang == "en":
            return sentence[0].upper() + sentence[1:] + "."
        else:
            return sentence + "ã€‚"
    
    @staticmethod
    def generate_lorem_ipsum(paragraphs: int = 1, sentences_per_paragraph: int = 5) -> str:
        """ç”ŸæˆLorem Ipsumå‡æ–‡"""
        lorem_ipsum = "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
        lorem_sentences = [s.strip() for s in lorem_ipsum.split('.') if s.strip()]
        
        result = []
        for p in range(paragraphs):
            paragraph_sentences = []
            for s in range(sentences_per_paragraph):
                sentence_idx = (p * sentences_per_paragraph + s) % len(lorem_sentences)
                paragraph_sentences.append(lorem_sentences[sentence_idx] + ".")
            result.append(" ".join(paragraph_sentences))
        
        return "\n\n".join(result)
    
    @staticmethod
    def generate_chinese_lorem(paragraphs: int = 1, sentences_per_paragraph: int = 5) -> str:
        """ç”Ÿæˆä¸­æ–‡å‡æ–‡"""
        chinese_lorem = "äººç”Ÿè€Œè‡ªç”±ï¼Œåœ¨å°Šä¸¥å’Œæƒåˆ©ä¸Šä¸€å¾‹å¹³ç­‰ã€‚ä»–ä»¬èµ‹æœ‰ç†æ€§å’Œè‰¯å¿ƒï¼Œå¹¶åº”ä»¥å…„å¼Ÿå…³ç³»çš„ç²¾ç¥ç›¸å¯¹å¾…ã€‚äººäººæœ‰èµ„æ ¼äº«æœ‰æœ¬å®£è¨€æ‰€è½½çš„ä¸€åˆ‡æƒåˆ©å’Œè‡ªç”±ï¼Œä¸åˆ†ç§æ—ã€è‚¤è‰²ã€æ€§åˆ«ã€è¯­è¨€ã€å®—æ•™ã€æ”¿æ²»æˆ–å…¶ä»–è§è§£ã€å›½ç±æˆ–ç¤¾ä¼šå‡ºèº«ã€è´¢äº§ã€å‡ºç”Ÿæˆ–å…¶ä»–èº«ä»½ç­‰ä»»ä½•åŒºåˆ«ã€‚å¹¶ä¸”ä¸åº”å½“ç”±äºä¸€ä¸ªäººæ‰€å±çš„å›½å®¶æˆ–é¢†åœŸçš„æ”¿æ²»çš„ã€è¡Œæ”¿çš„æˆ–è€…å›½é™…çš„åœ°ä½ä¹‹ä¸åŒè€Œæœ‰æ‰€åŒºåˆ«ï¼Œæ— è®ºè¯¥é¢†åœŸæ˜¯ç‹¬ç«‹é¢†åœŸã€æ‰˜ç®¡é¢†åœŸã€éè‡ªæ²»é¢†åœŸæˆ–è€…å¤„äºå…¶ä»–ä»»ä½•ä¸»æƒå—é™åˆ¶çš„æƒ…å†µä¹‹ä¸‹ã€‚"
        chinese_sentences = [s.strip() for s in chinese_lorem.split('ã€‚') if s.strip()]
        
        result = []
        for p in range(paragraphs):
            paragraph_sentences = []
            for s in range(sentences_per_paragraph):
                sentence_idx = (p * sentences_per_paragraph + s) % len(chinese_sentences)
                paragraph_sentences.append(chinese_sentences[sentence_idx] + "ã€‚")
            result.append("".join(paragraph_sentences))
        
        return "\n\n".join(result)


class TextAnalyzer:
    """æ–‡æœ¬åˆ†æå·¥å…·"""
    
    @staticmethod
    def count_words(text: str) -> int:
        """è®¡ç®—å•è¯æ•°é‡"""
        words = text.split()
        return len(words)
    
    @staticmethod
    def count_chinese_words(text: str) -> int:
        """è®¡ç®—ä¸­æ–‡è¯æ•°ï¼ˆä½¿ç”¨jiebaåˆ†è¯ï¼‰"""
        words = jieba.cut(text)
        return len(list(words))
    
    @staticmethod
    def count_characters(text: str, include_spaces: bool = False) -> int:
        """è®¡ç®—å­—ç¬¦æ•°"""
        if include_spaces:
            return len(text)
        else:
            return len(text.replace(" ", ""))
    
    @staticmethod
    def count_sentences(text: str) -> int:
        """è®¡ç®—å¥å­æ•°"""
        # è‹±æ–‡å’Œä¸­æ–‡çš„å¥å­ç»“æŸç¬¦
        pattern = r'[.ã€‚!ï¼?ï¼Ÿ]+(?:\s|$)'
        sentences = re.split(pattern, text)
        # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        sentences = [s for s in sentences if s.strip()]
        return len(sentences)
    
    @staticmethod
    def get_word_frequency(text: str, top_n: Optional[int] = None) -> Dict[str, int]:
        """è·å–å•è¯é¢‘ç‡"""
        words = text.split()
        counter = Counter(words)
        
        if top_n:
            return dict(counter.most_common(top_n))
        return dict(counter)
    
    @staticmethod
    def get_chinese_word_frequency(text: str, top_n: Optional[int] = None) -> Dict[str, int]:
        """è·å–ä¸­æ–‡è¯é¢‘ç‡"""
        words = jieba.cut(text)
        counter = Counter(words)
        
        if top_n:
            return dict(counter.most_common(top_n))
        return dict(counter)
    
    @staticmethod
    def calculate_text_similarity(text1: str, text2: str, method: str = "jaccard") -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        if method == "jaccard":
            # Jaccardç›¸ä¼¼åº¦ï¼šäº¤é›†å¤§å°é™¤ä»¥å¹¶é›†å¤§å°
            set1 = set(text1.split())
            set2 = set(text2.split())
            
            if not set1 and not set2:
                return 1.0
            
            intersection = len(set1.intersection(set2))
            union = len(set1.union(set2))
            
            return intersection / union
        
        elif method == "cosine":
            # ä½™å¼¦ç›¸ä¼¼åº¦çš„ç®€å•å®ç°
            from collections import Counter
            vec1 = Counter(text1.split())
            vec2 = Counter(text2.split())
            
            intersection = set(vec1.keys()) & set(vec2.keys())
            
            numerator = sum([vec1[x] * vec2[x] for x in intersection])
            
            sum1 = sum([vec1[x]**2 for x in vec1.keys()])
            sum2 = sum([vec2[x]**2 for x in vec2.keys()])
            denominator = (sum1 * sum2) ** 0.5
            
            if not denominator:
                return 0.0
            return numerator / denominator
        
        elif method == "levenshtein":
            # è±æ–‡æ–¯å¦è·ç¦»
            try:
                import Levenshtein
                distance = Levenshtein.distance(text1, text2)
                max_len = max(len(text1), len(text2))
                
                if max_len == 0:
                    return 1.0
                
                return 1 - (distance / max_len)
            except ImportError:
                logger.warning("Levenshtein æ¨¡å—æœªå®‰è£…ï¼Œå›é€€åˆ°åºåˆ—åŒ¹é…")
                method = "sequence"
        
        if method == "sequence":
            # åºåˆ—åŒ¹é…æ¯”ç‡
            matcher = difflib.SequenceMatcher(None, text1, text2)
            return matcher.ratio()
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•: {method}")
    
    @staticmethod
    def extract_numbers(text: str) -> List[float]:
        """æå–æ–‡æœ¬ä¸­çš„æ•°å­—"""
        pattern = r'-?\d+\.?\d*'
        matches = re.findall(pattern, text)
        return [float(match) for match in matches]
    
    @staticmethod
    def extract_emails(text: str) -> List[str]:
        """æå–æ–‡æœ¬ä¸­çš„é‚®ç®±åœ°å€"""
        pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
        return re.findall(pattern, text)
    
    @staticmethod
    def extract_urls(text: str) -> List[str]:
        """æå–æ–‡æœ¬ä¸­çš„URL"""
        pattern = r'https?://[^\s]+'
        return re.findall(pattern, text)
    
    @staticmethod
    def extract_chinese(text: str) -> str:
        """æå–ä¸­æ–‡å­—ç¬¦"""
        pattern = r'[\u4e00-\u9fa5]+'
        matches = re.findall(pattern, text)
        return ''.join(matches)
    
    @staticmethod
    def is_chinese_text(text: str, threshold: float = 0.5) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºä¸­æ–‡æ–‡æœ¬"""
        chinese_chars = re.findall(r'[\u4e00-\u9fa5]', text)
        chinese_ratio = len(chinese_chars) / len(text) if text else 0
        return chinese_ratio >= threshold
    
    @staticmethod
    def extract_dates(text: str) -> List[str]:
        """æå–æ–‡æœ¬ä¸­çš„æ—¥æœŸ"""
        # åŒ¹é…å¸¸è§æ—¥æœŸæ ¼å¼
        patterns = [
            r'\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}[æ—¥å·]?',  # 2023-01-01, 2023å¹´01æœˆ01æ—¥
            r'\d{1,2}[-/æœˆ]\d{1,2}[-/æ—¥å·]?\s*,?\s*\d{4}',  # 01-01-2023, 01æœˆ01æ—¥, 2023
            r'\d{1,2}(?:st|nd|rd|th)?\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+\d{4}'  # 1st Jan 2023
        ]
        
        results = []
        for pattern in patterns:
            matches = re.findall(pattern, text)
            results.extend(matches)
            
        return results
    
    @staticmethod
    def analyze_sentiment(text: str) -> Dict[str, float]:
        """ç®€å•çš„æƒ…æ„Ÿåˆ†æ"""
        try:
            from snownlp import SnowNLP
            s = SnowNLP(text)
            return {
                "positive_prob": s.sentiments,
                "negative_prob": 1 - s.sentiments
            }
        except ImportError:
            logger.warning("SnowNLP æ¨¡å—æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œæƒ…æ„Ÿåˆ†æ")
            # è¿”å›ä¸­æ€§æƒ…æ„Ÿ
            return {
                "positive_prob": 0.5,
                "negative_prob": 0.5
            }
    
    @staticmethod
    def get_text_readability(text: str, lang: str = "zh") -> Dict[str, float]:
        """è®¡ç®—æ–‡æœ¬å¯è¯»æ€§æŒ‡æ ‡"""
        result = {}
        
        if lang == "zh":
            # ä¸­æ–‡æ–‡æœ¬å¯è¯»æ€§
            sentences = TextAnalyzer.count_sentences(text)
            words = TextAnalyzer.count_chinese_words(text)
            chars = TextAnalyzer.count_characters(text, include_spaces=False)
            
            # å¹³å‡å¥é•¿
            if sentences > 0:
                avg_sentence_length = words / sentences
            else:
                avg_sentence_length = 0
            
            # å¹³å‡è¯é•¿
            if words > 0:
                avg_word_length = chars / words
            else:
                avg_word_length = 0
            
            # ç®€åŒ–çš„å¯è¯»æ€§æŒ‡æ ‡
            readability = 100 - (avg_sentence_length * 0.6 + avg_word_length * 5)
            
            result = {
                "avg_sentence_length": avg_sentence_length,
                "avg_word_length": avg_word_length,
                "readability": max(0, min(100, readability))  # é™åˆ¶åœ¨0-100èŒƒå›´å†…
            }
            
        else:
            # è‹±æ–‡æ–‡æœ¬å¯è¯»æ€§ (ç®€åŒ–çš„å¼—è±å¥‡æ˜“è¯»æ€§æŒ‡æ•°)
            sentences = TextAnalyzer.count_sentences(text)
            words = TextAnalyzer.count_words(text)
            syllables = TextAnalyzer._count_syllables(text)
            
            if words > 0 and sentences > 0:
                flesch_reading_ease = 206.835 - 1.015 * (words / sentences) - 84.6 * (syllables / words)
                flesch_kincaid_grade = 0.39 * (words / sentences) + 11.8 * (syllables / words) - 15.59
            else:
                flesch_reading_ease = 0
                flesch_kincaid_grade = 0
            
            result = {
                "flesch_reading_ease": max(0, min(100, flesch_reading_ease)),
                "flesch_kincaid_grade": max(0, flesch_kincaid_grade)
            }
        
        return result
    
    @staticmethod
    def _count_syllables(text: str) -> int:
        """è®¡ç®—è‹±æ–‡æ–‡æœ¬çš„éŸ³èŠ‚æ•°ï¼ˆä¼°è®¡å€¼ï¼‰"""
        # ç®€åŒ–çš„è®¡ç®—æ–¹æ³•
        text = text.lower()
        text = re.sub(r'[^a-z]', ' ', text)
        words = text.split()
        
        syllable_count = 0
        for word in words:
            word = word.strip()
            if not word:
                continue
                
            # ç‰¹æ®Šæƒ…å†µ: å•è¯ä»¥'e'ç»“å°¾ä½†ä¸æ˜¯'le'
            if word.endswith('e') and not word.endswith('le'):
                word = word[:-1]
                
            # è®¡ç®—å…ƒéŸ³ç»„
            vowels = "aeiouy"
            count = 0
            prev_is_vowel = False
            
            for char in word:
                is_vowel = char in vowels
                if is_vowel and not prev_is_vowel:
                    count += 1
                prev_is_vowel = is_vowel
                
            # ç¡®ä¿æ¯ä¸ªå•è¯è‡³å°‘æœ‰ä¸€ä¸ªéŸ³èŠ‚
            if count == 0:
                count = 1
                
            syllable_count += count
            
        return syllable_count


class MedicalTextProcessor:
    """åŒ»ç–—æ–‡æœ¬å¤„ç†å·¥å…·"""
    
    def __init__(self, medical_dict_path: Optional[str] = None):
        """åˆå§‹åŒ–åŒ»ç–—æ–‡æœ¬å¤„ç†å™¨"""
        self.segmenter = TextSegmenter(user_dict_path=medical_dict_path)
        
        # å¸¸è§åŒ»å­¦å•ä½è¯è¡¨
        self.medical_units = {
            "mg", "g", "kg", "ml", "l", "mmol", "mol", "mmHg", "kPa", "mm",
            "cm", "m", "IU", "U", "Î¼g", "ng", "pg", "mmol/L", "g/L", "mEq/L"
        }
        
        # å¸¸è§åŒ»ç–—æœ¯è¯­ç¼©å†™æ˜ å°„
        self.medical_abbr = {
            "BP": "è¡€å‹",
            "HR": "å¿ƒç‡",
            "RR": "å‘¼å¸é¢‘ç‡",
            "BT": "ä½“æ¸©",
            "SpO2": "è¡€æ°§é¥±å’Œåº¦",
            "WBC": "ç™½ç»†èƒè®¡æ•°",
            "RBC": "çº¢ç»†èƒè®¡æ•°",
            "Hb": "è¡€çº¢è›‹ç™½",
            "T": "ä½“æ¸©",
            "P": "è„‰æ",
            "R": "å‘¼å¸"
        }
        
        # åŠ è½½åŒ»å­¦è¯å…¸
        if medical_dict_path:
            self._load_medical_dict(medical_dict_path)
    
    def _load_medical_dict(self, dict_path: str) -> None:
        """åŠ è½½åŒ»å­¦è¯å…¸"""
        try:
            jieba.load_userdict(dict_path)
            logger.info(f"å·²åŠ è½½åŒ»å­¦è¯å…¸: {dict_path}")
        except Exception as e:
            logger.error(f"åŠ è½½åŒ»å­¦è¯å…¸å¤±è´¥: {e}")
    
    def segment_medical_text(self, text: str) -> List[str]:
        """åˆ†è¯åŒ»ç–—æ–‡æœ¬"""
        return self.segmenter.segment(text)
    
    def extract_medical_terms(self, text: str, term_dict: Optional[Dict[str, str]] = None) -> List[str]:
        """æå–åŒ»å­¦æœ¯è¯­"""
        # å¦‚æœæä¾›äº†æœ¯è¯­è¯å…¸ï¼Œä½¿ç”¨è¯å…¸åŒ¹é…
        if term_dict:
            terms = []
            for term in term_dict.keys():
                if term in text:
                    terms.append(term)
            return terms
        
        # å¦åˆ™ä½¿ç”¨å…³é”®è¯æå–
        return self.segmenter.extract_keywords(text)
    
    def normalize_medical_unit(self, text: str) -> str:
        """æ ‡å‡†åŒ–åŒ»ç–—å•ä½"""
        # åŒ¹é…æ•°å­—å’Œå•ä½çš„æ¨¡å¼
        number_unit_pattern = r'(\d+(?:\.\d+)?)\s*([a-zA-Z]+(?:/[a-zA-Z]+)?)'
        
        def replace_unit(match):
            number, unit = match.groups()
            # æ£€æŸ¥å•ä½æ˜¯å¦åœ¨åŒ»å­¦å•ä½åˆ—è¡¨ä¸­
            if unit.lower() in self.medical_units:
                # è¿”å›æ ‡å‡†æ ¼å¼ï¼šæ•°å­—å’Œå•ä½ä¹‹é—´æœ‰ä¸€ä¸ªç©ºæ ¼
                return f"{number} {unit}"
            return match.group(0)
        
        # æ›¿æ¢æ–‡æœ¬ä¸­çš„å•ä½æ ¼å¼
        return re.sub(number_unit_pattern, replace_unit, text)
    
    def expand_medical_abbreviations(self, text: str) -> str:
        """å±•å¼€åŒ»å­¦ç¼©å†™"""
        expanded_text = text
        
        # æ›¿æ¢ç¼©å†™
        for abbr, full in self.medical_abbr.items():
            # åªæ›¿æ¢ç‹¬ç«‹çš„ç¼©å†™è¯ï¼ˆå‰åæœ‰ç©ºæ ¼æˆ–æ ‡ç‚¹ï¼‰
            pattern = r'(?<![a-zA-Z])' + re.escape(abbr) + r'(?![a-zA-Z])'
            expanded_text = re.sub(pattern, f"{abbr}({full})", expanded_text)
        
        return expanded_text
    
    def extract_medical_values(self, text: str) -> Dict[str, List[Dict[str, Any]]]:
        """æå–åŒ»ç–—æ•°å€¼"""
        results = {}
        
        # æå–å„ç§åŒ»ç–—æ•°å€¼
        # 1. è¡€å‹
        bp_pattern = r'BP[ï¼š:]\s*(\d+)/(\d+)\s*(?:mmHg)?'
        bp_matches = re.finditer(bp_pattern, text)
        bp_results = []
        
        for match in bp_matches:
            systolic = int(match.group(1))
            diastolic = int(match.group(2))
            bp_results.append({
                "systolic": systolic,
                "diastolic": diastolic,
                "unit": "mmHg"
            })
        
        if bp_results:
            results["blood_pressure"] = bp_results
        
        # 2. ä½“æ¸©
        temp_pattern = r'(?:T|BT|ä½“æ¸©)[ï¼š:]\s*(\d+\.?\d*)\s*(?:â„ƒ|åº¦)'
        temp_matches = re.finditer(temp_pattern, text)
        temp_results = []
        
        for match in temp_matches:
            temperature = float(match.group(1))
            temp_results.append({
                "value": temperature,
                "unit": "â„ƒ"
            })
        
        if temp_results:
            results["temperature"] = temp_results
        
        # 3. è¡€ç³–
        glucose_pattern = r'è¡€ç³–[ï¼š:]\s*(\d+\.?\d*)\s*(?:mmol/L)'
        glucose_matches = re.finditer(glucose_pattern, text)
        glucose_results = []
        
        for match in glucose_matches:
            glucose = float(match.group(1))
            glucose_results.append({
                "value": glucose,
                "unit": "mmol/L"
            })
        
        if glucose_results:
            results["blood_glucose"] = glucose_results
        
        # 4. ä¸€èˆ¬åŒ¹é…æµ‹è¯•æ•°å€¼
        general_pattern = r'(\w+)\s*(?:[:ï¼š])\s*(\d+\.?\d*)\s*(?:[a-zA-Z]+(?:/[a-zA-Z]+)?)?'
        general_matches = re.finditer(general_pattern, text)
        
        for match in general_matches:
            test_name = match.group(1)
            value = float(match.group(2))
            
            # æ’é™¤å·²ç»å¤„ç†è¿‡çš„é¡¹ç›®
            if test_name not in ["BP", "T", "BT", "ä½“æ¸©", "è¡€ç³–"]:
                if test_name not in results:
                    results[test_name] = []
                
                results[test_name].append({
                    "value": value
                })
        
        return results
    
    def format_medical_report(self, report_data: Dict[str, Any], template: Optional[str] = None) -> str:
        """æ ¼å¼åŒ–åŒ»ç–—æŠ¥å‘Šæ•°æ®"""
        if not template:
            template = (
                "åŒ»ç–—æŠ¥å‘Š\n"
                "==============================\n"
                "{date}\n\n"
                "æ‚£è€…ä¿¡æ¯ï¼š\n"
                "å§“å: {patient_name}\n"
                "æ€§åˆ«: {patient_gender}\n"
                "å¹´é¾„: {patient_age}\n"
                "ID: {patient_id}\n\n"
                "æ£€æŸ¥ç»“æœï¼š\n"
                "{examination_results}\n\n"
                "è¯Šæ–­ï¼š\n"
                "{diagnosis}\n\n"
                "å»ºè®®ï¼š\n"
                "{recommendations}\n"
                "==============================\n"
                "åŒ»å¸ˆ: {doctor_name}"
            )
        
        # æ ¼å¼åŒ–æ£€æŸ¥ç»“æœ
        examination_results = ""
        if "examination_results" in report_data and isinstance(report_data["examination_results"], dict):
            for test, results in report_data["examination_results"].items():
                if isinstance(results, list):
                    for result in results:
                        value = result.get("value", "")
                        unit = result.get("unit", "")
                        examination_results += f"- {test}: {value}{' ' + unit if unit else ''}\n"
                else:
                    examination_results += f"- {test}: {results}\n"
        else:
            examination_results = report_data.get("examination_results", "æ— ")
        
        # ä½¿ç”¨æ¨¡æ¿æ›¿æ¢å­—æ®µ
        formatted_report = template
        for key, value in report_data.items():
            if key != "examination_results":  # å·²ç»å•ç‹¬å¤„ç†
                formatted_report = formatted_report.replace("{" + key + "}", str(value))
        
        # æ›¿æ¢æ£€æŸ¥ç»“æœ
        formatted_report = formatted_report.replace("{examination_results}", examination_results)
        
        # æ›¿æ¢ä»»ä½•é—æ¼çš„å­—æ®µ
        formatted_report = re.sub(r'\{[^}]+\}', 'æ— ', formatted_report)
        
        return formatted_report
    
    def detect_adverse_conditions(self, text: str, threshold_dict: Optional[Dict[str, Dict[str, float]]] = None) -> List[Dict[str, Any]]:
        """æ£€æµ‹å¼‚å¸¸åŒ»ç–—æŒ‡æ ‡"""
        # é»˜è®¤é˜ˆå€¼
        default_thresholds = {
            "blood_pressure": {"systolic_min": 90, "systolic_max": 140, "diastolic_min": 60, "diastolic_max": 90},
            "temperature": {"min": 36.0, "max": 37.3},
            "blood_glucose": {"min": 3.9, "max": 6.1}
        }
        
        # åˆå¹¶ç”¨æˆ·æä¾›çš„é˜ˆå€¼
        thresholds = default_thresholds.copy()
        if threshold_dict:
            for key, value in threshold_dict.items():
                if key in thresholds:
                    thresholds[key].update(value)
                else:
                    thresholds[key] = value
        
        # æå–åŒ»ç–—æ•°å€¼
        medical_values = self.extract_medical_values(text)
        
        # æ£€æµ‹å¼‚å¸¸
        abnormal_findings = []
        
        # è¡€å‹å¼‚å¸¸
        if "blood_pressure" in medical_values:
            for bp in medical_values["blood_pressure"]:
                systolic = bp.get("systolic")
                diastolic = bp.get("diastolic")
                
                if systolic and diastolic:
                    systolic_min = thresholds["blood_pressure"]["systolic_min"]
                    systolic_max = thresholds["blood_pressure"]["systolic_max"]
                    diastolic_min = thresholds["blood_pressure"]["diastolic_min"]
                    diastolic_max = thresholds["blood_pressure"]["diastolic_max"]
                    
                    if systolic < systolic_min or systolic > systolic_max or diastolic < diastolic_min or diastolic > diastolic_max:
                        abnormal_findings.append({
                            "type": "blood_pressure",
                            "value": f"{systolic}/{diastolic} mmHg",
                            "normal_range": f"{systolic_min}-{systolic_max}/{diastolic_min}-{diastolic_max} mmHg",
                            "is_high": systolic > systolic_max or diastolic > diastolic_max,
                            "is_low": systolic < systolic_min or diastolic < diastolic_min
                        })
        
        # ä½“æ¸©å¼‚å¸¸
        if "temperature" in medical_values:
            for temp in medical_values["temperature"]:
                value = temp.get("value")
                
                if value:
                    min_temp = thresholds["temperature"]["min"]
                    max_temp = thresholds["temperature"]["max"]
                    
                    if value < min_temp or value > max_temp:
                        abnormal_findings.append({
                            "type": "temperature",
                            "value": f"{value} â„ƒ",
                            "normal_range": f"{min_temp}-{max_temp} â„ƒ",
                            "is_high": value > max_temp,
                            "is_low": value < min_temp
                        })
        
        # è¡€ç³–å¼‚å¸¸
        if "blood_glucose" in medical_values:
            for glucose in medical_values["blood_glucose"]:
                value = glucose.get("value")
                
                if value:
                    min_glucose = thresholds["blood_glucose"]["min"]
                    max_glucose = thresholds["blood_glucose"]["max"]
                    
                    if value < min_glucose or value > max_glucose:
                        abnormal_findings.append({
                            "type": "blood_glucose",
                            "value": f"{value} mmol/L",
                            "normal_range": f"{min_glucose}-{max_glucose} mmol/L",
                            "is_high": value > max_glucose,
                            "is_low": value < min_glucose
                        })
        
        # æ£€æŸ¥å…¶ä»–å¯èƒ½çš„å¼‚å¸¸é¡¹ç›®
        for test_name, values in medical_values.items():
            if test_name not in ["blood_pressure", "temperature", "blood_glucose"] and test_name in thresholds:
                for value_obj in values:
                    value = value_obj.get("value")
                    
                    if value:
                        min_val = thresholds[test_name].get("min")
                        max_val = thresholds[test_name].get("max")
                        
                        if (min_val is not None and value < min_val) or (max_val is not None and value > max_val):
                            abnormal_findings.append({
                                "type": test_name,
                                "value": str(value),
                                "normal_range": f"{min_val if min_val is not None else '?'}-{max_val if max_val is not None else '?'}",
                                "is_high": max_val is not None and value > max_val,
                                "is_low": min_val is not None and value < min_val
                            })
        
        return abnormal_findings
    
    def extract_diagnoses(self, text: str) -> List[str]:
        """ä»åŒ»ç–—æ–‡æœ¬ä¸­æå–è¯Šæ–­ä¿¡æ¯"""
        diagnoses = []
        
        # å¸¸è§çš„è¯Šæ–­éƒ¨åˆ†æ ‡è¯†ç¬¦
        diagnosis_patterns = [
            r'è¯Šæ–­[ï¼š:]\s*(.+?)(?=\n|$)',
            r'åˆæ­¥è¯Šæ–­[ï¼š:]\s*(.+?)(?=\n|$)',
            r'ä¸´åºŠè¯Šæ–­[ï¼š:]\s*(.+?)(?=\n|$)',
            r'å‡ºé™¢è¯Šæ–­[ï¼š:]\s*(.+?)(?=\n|$)'
        ]
        
        for pattern in diagnosis_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                diagnosis = match.group(1).strip()
                if diagnosis:
                    # åˆ†å‰²å¤šä¸ªè¯Šæ–­
                    if "ï¼›" in diagnosis:
                        diagnoses.extend([d.strip() for d in diagnosis.split("ï¼›") if d.strip()])
                    elif ";" in diagnosis:
                        diagnoses.extend([d.strip() for d in diagnosis.split(";") if d.strip()])
                    elif "ï¼Œ" in diagnosis:
                        diagnoses.extend([d.strip() for d in diagnosis.split("ï¼Œ") if d.strip()])
                    elif "," in diagnosis:
                        diagnoses.extend([d.strip() for d in diagnosis.split(",") if d.strip()])
                    else:
                        diagnoses.append(diagnosis)
        
        # å»é‡
        return list(dict.fromkeys(diagnoses))
    
    def extract_medications(self, text: str) -> List[Dict[str, Any]]:
        """ä»åŒ»ç–—æ–‡æœ¬ä¸­æå–è¯ç‰©ä¿¡æ¯"""
        medications = []
        
        # åŒ¹é…è¯ç‰©åç§°ã€å‰‚é‡å’Œé¢‘ç‡
        med_pattern = r'(?:ç”¨è¯|å¤„æ–¹|åŒ»å˜±)[ï¼š:]\s*(.+?)(?=\n|$)'
        med_matches = re.finditer(med_pattern, text)
        
        for med_match in med_matches:
            med_text = med_match.group(1).strip()
            
            # åˆ†å‰²å¤šä¸ªè¯ç‰©
            med_items = re.split(r'[,ï¼Œ;ï¼›]', med_text)
            
            for item in med_items:
                item = item.strip()
                if not item:
                    continue
                
                # å°è¯•æå–è¯ç‰©åç§°ã€å‰‚é‡å’Œé¢‘ç‡
                name_dose_pattern = r'([^\d]+?)\s*(\d+\.?\d*\s*(?:mg|g|ml|mcg|Î¼g|IU|U|å•ä½))?(?:\s*([^ï¼Œ,]*?[æ¯æ—¥å¤©ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ¬¡))?'
                match = re.search(name_dose_pattern, item)
                
                if match:
                    name = match.group(1).strip() if match.group(1) else item
                    dose = match.group(2).strip() if match.group(2) else None
                    frequency = match.group(3).strip() if match.group(3) else None
                    
                    medications.append({
                        "name": name,
                        "dose": dose,
                        "frequency": frequency,
                        "raw_text": item
                    })
                else:
                    # å¦‚æœæ— æ³•è§£æï¼Œæ·»åŠ åŸå§‹æ–‡æœ¬
                    medications.append({
                        "name": item,
                        "raw_text": item
                    })
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ­£å¼çš„è¯ç‰©éƒ¨åˆ†ï¼Œå°è¯•ç›´æ¥ä»æ–‡æœ¬ä¸­æå–è¯ç‰©å
        if not medications:
            # å¸¸è§ä¸­è¥¿è¯ç‰©ååˆ—è¡¨
            common_meds = [
                "é˜¿å¸åŒ¹æ—", "å¸ƒæ´›èŠ¬", "å¯¹ä¹™é…°æ°¨åŸºé…š", "å¥¥ç¾æ‹‰å”‘", "äºŒç”²åŒèƒ", "åˆ©å°¿é…¸", 
                "è¾›ä¼ä»–æ±€", "æ°¨æ°¯åœ°å¹³", "æ°¯æ²™å¦", "ç”²çŠ¶è…ºç´ ", "èƒ°å²›ç´ ", "åœ°å¡ç±³æ¾",
                "è¥¿è¯", "ä¸­è¯", "æŠ—ç”Ÿç´ ", "æ¶ˆç‚è¯", "æ­¢ç—›è¯", "é€€çƒ§è¯", "è¡€å‹è¯"
            ]
            
            for med in common_meds:
                if med in text:
                    # å°è¯•æå–å®Œæ•´çš„è¯ç‰©ä¿¡æ¯
                    context = re.search(f"{med}[^ï¼Œã€‚,.\n]+", text)
                    if context:
                        medications.append({
                            "name": med,
                            "raw_text": context.group(0).strip()
                        })
        
        return medications
    
    def extract_patient_info(self, text: str) -> Dict[str, Any]:
        """ä»åŒ»ç–—æ–‡æœ¬ä¸­æå–æ‚£è€…åŸºæœ¬ä¿¡æ¯"""
        info = {}
        
        # æå–å§“å
        name_pattern = r'(?:å§“å|æ‚£è€…)[ï¼š:]\s*([^\s,ï¼Œã€‚.]+'
        name_match = re.search(name_pattern, text)
        if name_match:
            info["name"] = name_match.group(1).strip()
        
        # æå–æ€§åˆ«
        gender_pattern = r'æ€§åˆ«[ï¼š:]\s*(ç”·|å¥³)'
        gender_match = re.search(gender_pattern, text)
        if gender_match:
            info["gender"] = gender_match.group(1).strip()
        
        # æå–å¹´é¾„
        age_pattern = r'å¹´é¾„[ï¼š:]\s*(\d+)(?:\s*å²)?'
        age_match = re.search(age_pattern, text)
        if age_match:
            info["age"] = int(age_match.group(1).strip())
        
        # æå–æ‚£è€…ID
        id_pattern = r'(?:ID|ç—…å·|ç¼–å·|æ¡£æ¡ˆå·)[ï¼š:]\s*([A-Za-z0-9-]+)'
        id_match = re.search(id_pattern, text)
        if id_match:
            info["patient_id"] = id_match.group(1).strip()
        
        # æå–è”ç³»æ–¹å¼
        phone_pattern = r'(?:ç”µè¯|æ‰‹æœº|è”ç³»æ–¹å¼)[ï¼š:]\s*(\d{3,}[-\s]?\d{4,}(?:[-\s]?\d{4})?)'
        phone_match = re.search(phone_pattern, text)
        if phone_match:
            info["phone"] = phone_match.group(1).strip()
        
        # æå–å°±è¯Šæ—¥æœŸ
        date_pattern = r'(?:æ—¥æœŸ|å°±è¯Šæ—¥æœŸ|é—¨è¯Šæ—¥æœŸ)[ï¼š:]\s*(\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}[æ—¥]?)'
        date_match = re.search(date_pattern, text)
        if date_match:
            info["visit_date"] = date_match.group(1).strip()
        
        return info
    
    def generate_medical_question_embeddings(self, questions: List[str]) -> Dict[str, List[str]]:
        """
        ä¸ºåŒ»ç–—é—®é¢˜ç”Ÿæˆè¯­ä¹‰ç›¸å…³çš„é—®é¢˜å˜ä½“åµŒå…¥
        ç”¨äºå¢å¼ºåŒ»ç–—é—®ç­”ç³»ç»Ÿçš„æ£€ç´¢
        """
        question_variants = {}
        
        for question in questions:
            variants = []
            
            # æ·»åŠ åŸå§‹é—®é¢˜
            variants.append(question)
            
            # æå–å…³é”®è¯
            keywords = self.segmenter.extract_keywords(question, topK=5)
            
            # å˜ä½“1: ä½¿ç”¨å…³é”®è¯æ„å»ºé—®é¢˜
            if keywords:
                keywords_question = "å…³äº" + "å’Œ".join(keywords[:3]) + "çš„ä¿¡æ¯"
                variants.append(keywords_question)
            
            # å˜ä½“2: æ›´æ”¹é—®å¥å½¢å¼
            # å°†"æ˜¯ä»€ä¹ˆ"æ”¹ä¸º"æœ‰å“ªäº›"
            variants.append(re.sub(r'æ˜¯ä»€ä¹ˆ', r'æœ‰å“ªäº›', question))
            # å°†"å¦‚ä½•"æ”¹ä¸º"æ€ä¹ˆæ ·"
            variants.append(re.sub(r'å¦‚ä½•', r'æ€ä¹ˆæ ·', question))
            
            # å˜ä½“3: æ·»åŠ åŒ»ç–—æœ¯è¯­
            # å°†"å¤´ç—›"æ”¹ä¸º"å¤´ç—›(å¤´ç–¼)"
            variants.append(re.sub(r'å¤´ç—›', r'å¤´ç—›(å¤´ç–¼)', question))
            # å°†"æ„Ÿå†’"æ”¹ä¸º"æ„Ÿå†’(ä¸Šå‘¼å¸é“æ„ŸæŸ“)"
            variants.append(re.sub(r'æ„Ÿå†’', r'æ„Ÿå†’(ä¸Šå‘¼å¸é“æ„ŸæŸ“)', question))
            
            # å­˜å‚¨å˜ä½“
            question_variants[question] = list(set(variants))
        
        return question_variants
    
    def classify_medical_text(self, text: str) -> Dict[str, float]:
        """
        å¯¹åŒ»ç–—æ–‡æœ¬è¿›è¡Œåˆ†ç±»
        è¿”å›å„ç±»åˆ«çš„ç½®ä¿¡åº¦åˆ†æ•°
        """
        # åŒ»ç–—æ–‡æœ¬åˆ†ç±»ç‰¹å¾è¯
        category_features = {
            "ç—‡çŠ¶æè¿°": ["ç–¼ç—›", "ä¸é€‚", "æ„Ÿè§‰", "å‡ºç°", "ç—‡çŠ¶", "æ„Ÿåˆ°"],
            "ç–¾ç—…ä¿¡æ¯": ["ç–¾ç—…", "ç—…å› ", "ç—…ç†", "æœºåˆ¶", "å½±å“", "å‘ç—…ç‡"],
            "æ²»ç–—æ–¹æ¡ˆ": ["æ²»ç–—", "æ‰‹æœ¯", "è¯ç‰©", "æ–¹æ¡ˆ", "æ•ˆæœ", "ç–—æ³•", "ç”¨è¯"],
            "é¢„é˜²ä¿å¥": ["é¢„é˜²", "ä¿å¥", "æŠ¤ç†", "é¥®é£Ÿ", "å»ºè®®", "ç”Ÿæ´»æ–¹å¼"],
            "æ£€æŸ¥æ£€éªŒ": ["æ£€æŸ¥", "åŒ–éªŒ", "å½±åƒ", "ç»“æœ", "æŠ¥å‘Š", "CT", "æ ¸ç£", "Bè¶…"]
        }
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„åˆ†æ•°
        category_scores = {}
        
        for category, features in category_features.items():
            score = 0
            for feature in features:
                if feature in text:
                    score += 1
            
            # å½’ä¸€åŒ–åˆ†æ•°
            category_scores[category] = score / len(features)
        
        # ç¡®ä¿åˆ†æ•°æ€»å’Œä¸º1
        total_score = sum(category_scores.values())
        if total_score > 0:
            for category in category_scores:
                category_scores[category] /= total_score
        
        return category_scores


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ–‡æœ¬æ ‡å‡†åŒ–ç¤ºä¾‹
    text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼ŒåŒ…å«HTMLæ ‡ç­¾<p>å’ŒURL http://example.comï¼Œè¿˜æœ‰emojiğŸ˜Šã€‚"
    normalizer = TextNormalizer()
    normalized_text = normalizer.normalize_text(text)
    print(f"æ ‡å‡†åŒ–æ–‡æœ¬: {normalized_text}")
    
    # åˆ†è¯ç¤ºä¾‹
    segmenter = TextSegmenter()
    tokens = segmenter.segment("æˆ‘ä»Šå¤©å»åŒ»é™¢çœ‹äº†åŒ»ç”Ÿï¼ŒåŒ»ç”Ÿè¯´æˆ‘çš„è¡€å‹æœ‰ç‚¹é«˜ã€‚")
    print(f"åˆ†è¯ç»“æœ: {tokens}")
    
    # åŒ»ç–—æ–‡æœ¬å¤„ç†ç¤ºä¾‹
    medical_processor = MedicalTextProcessor()
    medical_text = "æ‚£è€…ç”·ï¼Œ45å²ï¼Œè¡€å‹BP: 145/95 mmHgï¼Œä½“æ¸©T: 37.6â„ƒï¼Œè¡€ç³–: 6.5 mmol/Lã€‚"
    extracted_values = medical_processor.extract_medical_values(medical_text)
    print(f"æå–çš„åŒ»ç–—æ•°å€¼: {extracted_values}")
    
    abnormal_findings = medical_processor.detect_adverse_conditions(medical_text)
    print(f"æ£€æµ‹åˆ°çš„å¼‚å¸¸çŠ¶å†µ: {abnormal_findings}")